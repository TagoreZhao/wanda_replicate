{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import heapq \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from lib.sparsegpt import SparseGPT \n",
    "from lib.layerwrapper import WrappedGPT\n",
    "from lib.ablate import AblateGPT \n",
    "import argparse\n",
    "import os \n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from importlib.metadata import version\n",
    "from lib.prune import prune_wanda, prune_magnitude, prune_sparsegpt, prune_ablate, check_sparsity, find_layers\n",
    "from lib.eval import eval_ppl, eval_zero_shot\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for tokenized input IDs\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "def prepare_calibration_input(model, dataloader, device):\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # Set the appropriate device if \"embed_tokens\" is mapped\n",
    "    if \"model.embed_tokens\" in model.hf_device_map:\n",
    "        device = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps_cpu = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=\"cpu\")\n",
    "    outs_cpu = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=\"cpu\")\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps_cpu[cache['i']] = inp.cpu()  # Store on CPU to save GPU memory\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask'].cpu()\n",
    "            cache['position_ids'] = kwargs['position_ids'].cpu()\n",
    "            raise ValueError\n",
    "\n",
    "    # Wrap the first layer to intercept input/output\n",
    "    layers[0] = Catcher(layers[0])\n",
    "\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            batch_gpu = batch[0].to(device)\n",
    "            model(batch_gpu)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache after each batch\n",
    "\n",
    "    # Restore the original model layer\n",
    "    layers[0] = layers[0].module\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    # Move only necessary data to GPU in a batched manner later in the pipeline\n",
    "    return inps_cpu, outs_cpu, cache['attention_mask'], cache['position_ids']\n",
    "\n",
    "\n",
    "def find_layers(module, layers=[nn.Linear], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find the layers of a certain type in a module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): PyTorch module.\n",
    "        layers (list): List of layer types to find.\n",
    "        name (str): Name of the module.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layers of the given type(s) within the module.\n",
    "    \"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(\n",
    "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
    "        ))\n",
    "    return res\n",
    "\n",
    "def return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before):\n",
    "    thres_cumsum = sum_before * alpha \n",
    "    sort_mask = tmp_metric <= thres_cumsum.reshape((-1,1))\n",
    "    thres = torch.gather(sort_res[0], dim=1, index=sort_mask.sum(dim=1, keepdims=True)-1)\n",
    "    W_mask = (W_metric <= thres)\n",
    "    cur_sparsity = (W_mask==True).sum() / W_mask.numel()\n",
    "    return W_mask, cur_sparsity\n",
    "\n",
    "def prune_wanda(args, model, tokenizer, device=torch.device(\"cuda:0\"), prune_n=0, prune_m=0):\n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "\n",
    "    print(\"loading calibdation data\")\n",
    "    dataloader, _ = get_loaders(\"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)\n",
    "    print(\"dataset loading complete\")\n",
    "    with torch.no_grad():\n",
    "        inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "\n",
    "    layers = model.model.layers\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        if f\"model.layers.{i}\" in model.hf_device_map:   ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            inps, outs, attention_mask, position_ids = inps.to(dev), outs.to(dev), attention_mask.to(dev), position_ids.to(dev)\n",
    "\n",
    "        wrapped_layers = {}\n",
    "        for name in subset:\n",
    "            wrapped_layers[name] = WrappedGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in wrapped_layers:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        for name in subset:\n",
    "            print(f\"pruning layer {i} name {name}\")\n",
    "            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "\n",
    "            W_mask = (torch.zeros_like(W_metric) == 1)  ## initialize a mask to be all False\n",
    "            if prune_n != 0:\n",
    "                # structured n:m sparsity\n",
    "                for ii in range(W_metric.shape[1]):\n",
    "                    if ii % prune_m == 0:\n",
    "                        tmp = W_metric[:,ii:(ii+prune_m)].float()\n",
    "                        W_mask.scatter_(1,ii+torch.topk(tmp, prune_n,dim=1, largest=False)[1], True)\n",
    "            else:\n",
    "                sort_res = torch.sort(W_metric, dim=-1, stable=True)\n",
    "\n",
    "                if args.use_variant:\n",
    "                    # wanda variant \n",
    "                    tmp_metric = torch.cumsum(sort_res[0], dim=1)\n",
    "                    sum_before = W_metric.sum(dim=1)\n",
    "\n",
    "                    alpha = 0.4\n",
    "                    alpha_hist = [0., 0.8]\n",
    "                    W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)\n",
    "                    while (torch.abs(cur_sparsity - args.sparsity_ratio)>0.001) and (alpha_hist[1]-alpha_hist[0]>=0.001):\n",
    "                        if cur_sparsity > args.sparsity_ratio:\n",
    "                            alpha_new = (alpha + alpha_hist[0]) / 2.0\n",
    "                            alpha_hist[1] = alpha\n",
    "                        else:\n",
    "                            alpha_new = (alpha + alpha_hist[1]) / 2.0\n",
    "                            alpha_hist[0] = alpha\n",
    "\n",
    "                        alpha = alpha_new \n",
    "                        W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)\n",
    "                    print(f\"alpha found {alpha} sparsity {cur_sparsity:.6f}\")\n",
    "                else:\n",
    "                    # unstructured pruning\n",
    "                    indices = sort_res[1][:,:int(W_metric.shape[1]*args.sparsity_ratio)]\n",
    "                    W_mask.scatter_(1, indices, True)\n",
    "\n",
    "            subset[name].weight.data[W_mask] = 0  ## set weights to zero \n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    model.config.use_cache = use_cache \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_llm(model_name, cache_dir=\"llm_weights\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        torch_dtype=torch.float16, \n",
    "        cache_dir=cache_dir,  # Use the specified cache directory\n",
    "        low_cpu_mem_usage=True, \n",
    "        device_map=\"auto\",\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    model.seqlen = model.config.max_position_embeddings\n",
    "    return model\n",
    "\n",
    "# Load and process c4 dataset\n",
    "def get_c4(nsamples, seed, seqlen, tokenizer):\n",
    "    # Load train and validation datasets with streaming\n",
    "    traindata = load_dataset('allenai/c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')\n",
    "    valdata = load_dataset('allenai/c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')\n",
    "\n",
    "    # Initialize random seed for sample selection\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Generator to yield tokenized batches on-the-fly\n",
    "    def data_generator():\n",
    "        for _ in range(nsamples):\n",
    "            while True:\n",
    "                # Sample a random document and tokenize it\n",
    "                i = random.randint(0, len(traindata) - 1)\n",
    "                trainenc = tokenizer(traindata[i]['text'], return_tensors='pt', truncation=True, max_length=seqlen)\n",
    "                if trainenc.input_ids.shape[1] >= seqlen:\n",
    "                    break\n",
    "            # Slice a sequence-length section randomly from tokenized data\n",
    "            start = random.randint(0, trainenc.input_ids.shape[1] - seqlen)\n",
    "            input_ids = trainenc.input_ids[:, start:start + seqlen]\n",
    "            labels = input_ids.clone()\n",
    "            labels[:, :-1] = -100  # Only predict next token\n",
    "            \n",
    "            yield (input_ids, labels)\n",
    "\n",
    "    # Wrap generator to provide trainloader and test data\n",
    "    trainloader = list(data_generator())  # Lazily load samples\n",
    "    valenc = tokenizer(' '.join(valdata[:100]['text']), return_tensors='pt', truncation=True, max_length=seqlen)\n",
    "    valenc = TokenizerWrapper(valenc.input_ids)\n",
    "    \n",
    "    return trainloader, valenc\n",
    "\n",
    "# Function to select the appropriate loader based on dataset name\n",
    "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
    "    if 'wikitext2' in name:\n",
    "        return get_wikitext2(nsamples, seed, seqlen, tokenizer)\n",
    "    if \"c4\" in name:\n",
    "        return get_c4(nsamples, seed, seqlen, tokenizer)\n",
    "    \n",
    "# Load and process wikitext2 dataset\n",
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    # Load train and test datasets\n",
    "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    # Encode datasets\n",
    "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "\n",
    "    # Generate samples from training set\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prepare_calibration_input_initialization():\n",
    "    model = get_llm(\"baffo32/decapoda-research-llama-7B-hf\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\", use_fast=False)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dataloader, _ = get_loaders(\"c4\", nsamples=4, seed=0, seqlen=model.seqlen, tokenizer=tokenizer)\n",
    "\n",
    "    inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "    assert inps.shape == (128, model.seqlen, model.config.hidden_size), \"Incorrect shape for `inps`\"\n",
    "    assert outs.shape == (128, model.seqlen, model.config.hidden_size), \"Incorrect shape for `outs`\"\n",
    "    assert attention_mask is not None, \"Attention mask should not be None\"\n",
    "    assert position_ids is not None, \"Position IDs should not be None\"\n",
    "\n",
    "def test_prepare_calibration_input_device():\n",
    "    cache_dir = \"llm_weights\"  # Specify your cache directory for local loading\n",
    "    model = get_llm(\"baffo32/decapoda-research-llama-7B-hf\", cache_dir=cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\", use_fast=False, cache_dir=cache_dir)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    dataloader, _ = get_loaders(\"c4\", nsamples=4, seed=0, seqlen=model.seqlen, tokenizer=tokenizer)\n",
    "\n",
    "    # Run prepare_calibration_input\n",
    "    inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)\n",
    "    \n",
    "    # Check that the inps and outs tensors are on the CPU, as defined in prepare_calibration_input\n",
    "    assert inps.device == torch.device(\"cpu\"), \"Input tensor `inps` should be on the CPU initially\"\n",
    "    assert outs.device == torch.device(\"cpu\"), \"Output tensor `outs` should be on the CPU initially\"\n",
    "    \n",
    "    # Check that the attention mask and position IDs are also on the CPU\n",
    "    assert attention_mask.device == torch.device(\"cpu\"), \"Attention mask should be on the CPU\"\n",
    "    assert position_ids.device == torch.device(\"cpu\"), \"Position IDs should be on the CPU\"\n",
    "    \n",
    "    # Check shapes to ensure tensors are correctly sized\n",
    "    assert inps.shape == (128, model.seqlen, model.config.hidden_size), \"Incorrect shape for `inps`\"\n",
    "    assert outs.shape == (128, model.seqlen, model.config.hidden_size), \"Incorrect shape for `outs`\"\n",
    "    assert attention_mask is not None, \"Attention mask should not be None\"\n",
    "    assert position_ids is not None, \"Position IDs should not be None\"\n",
    "\n",
    "    print(\"test_prepare_calibration_input_device passed.\")\n",
    "\n",
    "def test_find_layers():\n",
    "    cache_dir = \"llm_weights\"  # Specify your cache directory for local loading\n",
    "    model = get_llm(\"baffo32/decapoda-research-llama-7B-hf\", cache_dir=cache_dir)\n",
    "    layers = find_layers(model, layers=[nn.Linear])\n",
    "\n",
    "    assert isinstance(layers, dict), \"Output should be a dictionary\"\n",
    "    assert len(layers) > 0, \"No layers found when expected\"\n",
    "    for name, layer in layers.items():\n",
    "        assert isinstance(layer, nn.Linear), f\"Layer `{name}` is not of type `nn.Linear`\"\n",
    "        \n",
    "def test_prune_wanda_sparsity():\n",
    "    args = argparse.Namespace(nsamples=4, seed=0, use_variant=False, sparsity_ratio=0.5, prune_method='wanda')\n",
    "    cache_dir = \"llm_weights\"  # Specify your cache directory for local loading\n",
    "    model = get_llm(\"baffo32/decapoda-research-llama-7B-hf\", cache_dir=cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    prune_wanda(args, model, tokenizer, device=device)\n",
    "\n",
    "    for name, layer in find_layers(model).items():\n",
    "        sparsity = (layer.weight == 0).float().mean().item()\n",
    "        assert abs(sparsity - args.sparsity_ratio) < 0.05, f\"Sparsity mismatch in layer `{name}`\"\n",
    "\n",
    "def test_prune_wanda_weight_modification():\n",
    "    args = argparse.Namespace(nsamples=4, seed=0, use_variant=False, sparsity_ratio=0.5, prune_method='wanda')\n",
    "    cache_dir = \"llm_weights\"  # Specify your cache directory for local loading\n",
    "    model = get_llm(\"baffo32/decapoda-research-llama-7B-hf\", cache_dir=cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    # Capture original weights before pruning\n",
    "    original_weights = {name: layer.weight.clone() for name, layer in find_layers(model).items()}\n",
    "\n",
    "    prune_wanda(args, model, tokenizer, device=device)\n",
    "\n",
    "    for name, layer in find_layers(model).items():\n",
    "        modified_weights = layer.weight\n",
    "        assert torch.sum(original_weights[name] != modified_weights) > 0, f\"Layer `{name}` was not modified\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/conda/envs/prune_llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:56<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading calibration data\n",
      "Dataset loading complete\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'dev' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_prune_wanda_sparsity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 54\u001b[0m, in \u001b[0;36mtest_prune_wanda_sparsity\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaffo32/decapoda-research-llama-7B-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mprune_wanda\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m find_layers(model)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     57\u001b[0m     sparsity \u001b[38;5;241m=\u001b[39m (layer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[27], line 118\u001b[0m, in \u001b[0;36mprune_wanda\u001b[0;34m(args, model, tokenizer, device, prune_n, prune_m)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mnsamples):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 118\u001b[0m         outs[j] \u001b[38;5;241m=\u001b[39m layer(inps[j]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdev\u001b[49m), attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, position_ids\u001b[38;5;241m=\u001b[39mposition_ids)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Remove hooks to free memory\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handles:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dev' referenced before assignment"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_prune_wanda_sparsity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
