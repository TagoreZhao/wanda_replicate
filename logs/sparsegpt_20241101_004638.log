/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
torch 1.10.1
transformers 4.46.1
accelerate 1.0.1
# of gpus:  1
loading llm model baffo32/decapoda-research-llama-7B-hf
Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]Downloading shards:   3%|▎         | 1/33 [00:02<01:18,  2.47s/it]Downloading shards:   6%|▌         | 2/33 [00:04<01:15,  2.44s/it]Downloading shards:   9%|▉         | 3/33 [00:07<01:12,  2.42s/it]Downloading shards:  12%|█▏        | 4/33 [00:09<01:11,  2.45s/it]Downloading shards:  15%|█▌        | 5/33 [00:15<01:41,  3.62s/it]Downloading shards:  18%|█▊        | 6/33 [00:18<01:28,  3.27s/it]Downloading shards:  21%|██        | 7/33 [00:23<01:46,  4.11s/it]Downloading shards:  24%|██▍       | 8/33 [00:26<01:29,  3.59s/it]Downloading shards:  27%|██▋       | 9/33 [00:29<01:19,  3.33s/it]Downloading shards:  30%|███       | 10/33 [00:31<01:09,  3.04s/it]Downloading shards:  33%|███▎      | 11/33 [00:36<01:23,  3.79s/it]Downloading shards:  36%|███▋      | 12/33 [00:39<01:11,  3.40s/it]Downloading shards:  39%|███▉      | 13/33 [00:41<01:01,  3.10s/it]Downloading shards:  42%|████▏     | 14/33 [00:44<00:54,  2.88s/it]Downloading shards:  45%|████▌     | 15/33 [00:46<00:49,  2.77s/it]Downloading shards:  48%|████▊     | 16/33 [00:49<00:45,  2.67s/it]Downloading shards:  52%|█████▏    | 17/33 [00:51<00:41,  2.58s/it]Downloading shards:  55%|█████▍    | 18/33 [00:54<00:38,  2.53s/it]Downloading shards:  58%|█████▊    | 19/33 [00:56<00:35,  2.55s/it]Downloading shards:  61%|██████    | 20/33 [00:58<00:32,  2.51s/it]Downloading shards:  64%|██████▎   | 21/33 [01:01<00:29,  2.48s/it]Downloading shards:  67%|██████▋   | 22/33 [01:03<00:27,  2.46s/it]Downloading shards:  70%|██████▉   | 23/33 [01:06<00:24,  2.44s/it]Downloading shards:  73%|███████▎  | 24/33 [01:11<00:30,  3.37s/it]Downloading shards:  76%|███████▌  | 25/33 [01:14<00:24,  3.11s/it]Downloading shards:  79%|███████▉  | 26/33 [01:19<00:26,  3.79s/it]Downloading shards:  82%|████████▏ | 27/33 [01:21<00:20,  3.37s/it]Downloading shards:  85%|████████▍ | 28/33 [01:27<00:20,  4.01s/it]Downloading shards:  88%|████████▊ | 29/33 [01:33<00:18,  4.58s/it]Downloading shards:  91%|█████████ | 30/33 [01:35<00:11,  3.97s/it]Downloading shards:  94%|█████████▍| 31/33 [01:38<00:07,  3.57s/it]Downloading shards:  97%|█████████▋| 32/33 [01:40<00:03,  3.24s/it]Downloading shards: 100%|██████████| 33/33 [01:44<00:00,  3.18s/it]Downloading shards: 100%|██████████| 33/33 [01:44<00:00,  3.15s/it]
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:568: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:06<03:41,  6.91s/it]Loading checkpoint shards:   6%|▌         | 2/33 [00:13<03:33,  6.89s/it]Loading checkpoint shards:   9%|▉         | 3/33 [00:20<03:27,  6.93s/it]Loading checkpoint shards:  12%|█▏        | 4/33 [00:27<03:20,  6.92s/it]Loading checkpoint shards:  15%|█▌        | 5/33 [00:34<03:13,  6.93s/it]Loading checkpoint shards:  18%|█▊        | 6/33 [00:41<03:06,  6.92s/it]Loading checkpoint shards:  21%|██        | 7/33 [00:48<03:00,  6.93s/it]Loading checkpoint shards:  24%|██▍       | 8/33 [00:55<02:52,  6.90s/it]Loading checkpoint shards:  27%|██▋       | 9/33 [01:02<02:45,  6.91s/it]Loading checkpoint shards:  30%|███       | 10/33 [01:09<02:39,  6.92s/it]Loading checkpoint shards:  33%|███▎      | 11/33 [01:16<02:32,  6.93s/it]Loading checkpoint shards:  36%|███▋      | 12/33 [01:23<02:25,  6.93s/it]Loading checkpoint shards:  39%|███▉      | 13/33 [01:30<02:18,  6.94s/it]Loading checkpoint shards:  42%|████▏     | 14/33 [01:36<02:11,  6.94s/it]Loading checkpoint shards:  45%|████▌     | 15/33 [01:43<02:04,  6.94s/it]Loading checkpoint shards:  48%|████▊     | 16/33 [01:50<01:57,  6.89s/it]Loading checkpoint shards:  52%|█████▏    | 17/33 [01:57<01:49,  6.85s/it]Loading checkpoint shards:  55%|█████▍    | 18/33 [02:04<01:42,  6.81s/it]Loading checkpoint shards:  55%|█████▍    | 18/33 [02:10<01:48,  7.26s/it]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.Traceback (most recent call    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_re    model = get_llm(args.model, args.c    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/e    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 6.99 GiB already allocated; 14.25 MiB free; 7.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
id fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
