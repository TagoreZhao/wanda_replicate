Running wanda pruning with MPI on a single node with 4 GPUs
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
transformers 4.46.1
transformers 4.46.1
transformerstransformers  4.46.14.46.1

accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.19s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.02s/it]
Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 169, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 201, in get_spm_processor
    model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py", line 38, in import_protobuf
    from sentencepiece import sentencepiece_model_pb2
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/sentencepiece/sentencepiece_model_pb2.py", line 5, in <module>
    from google.protobuf.internal import builder as _builder
ModuleNotFoundError: No module named 'google'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 59, in main
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py", line 920, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2448, in _from_pretrained
    except import_protobuf_decode_error():
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 169, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 201, in get_spm_processor
    model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py", line 38, in import_protobuf
    from sentencepiece import sentencepiece_model_pb2
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/sentencepiece/sentencepiece_model_pb2.py", line 5, in <module>
    from google.protobuf.internal import builder as _builder
ModuleNotFoundError: No module named 'google'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 59, in main
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py", line 920, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2448, in _from_pretrained
    except import_protobuf_decode_error():
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 169, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 201, in get_spm_processor
    model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py", line 38, in import_protobuf
    from sentencepiece import sentencepiece_model_pb2
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/sentencepiece/sentencepiece_model_pb2.py", line 5, in <module>
    from google.protobuf.internal import builder as _builder
ModuleNotFoundError: No module named 'google'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 59, in main
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py", line 920, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2448, in _from_pretrained
    except import_protobuf_decode_error():
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 169, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama.py", line 201, in get_spm_processor
    model_pb2 = import_protobuf(f"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)")
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py", line 38, in import_protobuf
    from sentencepiece import sentencepiece_model_pb2
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/sentencepiece/sentencepiece_model_pb2.py", line 5, in <module>
    from google.protobuf.internal import builder as _builder
ModuleNotFoundError: No module named 'google'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 59, in main
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py", line 920, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2448, in _from_pretrained
    except import_protobuf_decode_error():
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3918255 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3918253) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.10.1', 'console_scripts', 'torchrun')())
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/run.py", line 719, in main
    run(args)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-11-01_02:51:37
  host      : gpu01.hpc.at.sfsu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3918254)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-11-01_02:51:37
  host      : gpu01.hpc.at.sfsu.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3918256)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-01_02:51:37
  host      : gpu01.hpc.at.sfsu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3918253)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[57115,1],3]
  Exit code:    1
--------------------------------------------------------------------------
Finished wanda pruning with MPI on a single node
