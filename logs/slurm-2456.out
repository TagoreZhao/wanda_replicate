Running wanda pruning with MPI on a single node with 4 GPUs
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1transformers
 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerateaccelerate  1.0.11.0.1

accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]dataset loading complete
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]
use device  cuda:0
pruning starts
loading calibdation data
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.67s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.31s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.29s/it]
dataset loading complete
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.54s/it]pruning layer 2 name self_attn.q_proj
pruning layer 2 name self_attn.k_proj
pruning layer 2 name self_attn.v_proj
pruning layer 2 name self_attn.o_proj
pruning layer 2 name mlp.gate_proj
pruning layer 2 name mlp.up_proj
pruning layer 2 name mlp.down_proj
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]use device  cuda:0
pruning starts
loading calibdation data
pruning layer 3 name self_attn.q_proj
pruning layer 3 name self_attn.k_proj
pruning layer 3 name self_attn.v_proj
pruning layer 3 name self_attn.o_proj
pruning layer 3 name mlp.gate_proj
pruning layer 3 name mlp.up_proj
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]pruning layer 3 name mlp.down_proj
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.20s/it]use device  cuda:0
pruning starts
loading calibdation data
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]
dataset loading complete
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 69, in main
    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
  File "/Users/918839576/Trepo/wanda_replicate/lib/prune.py", line 180, in prune_wanda
    inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device)
  File "/Users/918839576/Trepo/wanda_replicate/lib/prune.py", line 68, in prepare_calibration_input
    inps = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=device)
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 79.14 GiB total capacity; 2.88 GiB already allocated; 983.31 MiB free; 2.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
Loading checkpoint shards:   0%|          | 0/2 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 722.02 MiB already allocated; 11.31 MiB free; 724.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 250.01 MiB already allocated; 11.31 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 13.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 15.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:10<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:10<?, ?it/s]

Loading checkpoint shards:   0%|          | 0/2 [00:10<?, ?it/s]
Traceback (most recent call last):
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
        main()main()

  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
      File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 23.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 23.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 23.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 23.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927725 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927728 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927736 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927726 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927727 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927734 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927723 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927729 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927733 closing signal SIGTERM
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
dataset loading complete
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 3927738) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 3927732) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 3927737) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927724 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927730 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927731 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3927735 closing signal SIGTERM
torch 1.10.1
torch 1.10.1
torch 1.10.1
torchtorch  1.10.11.10.1

torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
transformers 4.46.1
transformerstransformers  4.46.14.46.1

transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
# of gpus:  4
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.92s/it]
use device  cuda:0
pruning starts
loading calibdation data
dataset loading complete
dataset loading complete
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]dataset loading complete
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]
use device  cuda:0
pruning starts
loading calibdation data
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 636.02 MiB already allocated; 59.31 MiB free; 638.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 808.02 MiB already allocated; 59.31 MiB free; 810.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 722.02 MiB already allocated; 59.31 MiB free; 724.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
pruning layer 1 name mlp.down_proj
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 47.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 45.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 69, in main
    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
  File "/Users/918839576/Trepo/wanda_replicate/lib/prune.py", line 205, in prune_wanda
    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 339, in forward
    query_states = self.q_proj(hidden_states)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1120, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 11.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 11.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 15.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 15.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 17.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 19.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
pruning layer 0 name self_attn.q_proj
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 21.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 69, in main
    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
  File "/Users/918839576/Trepo/wanda_replicate/lib/prune.py", line 251, in prune_wanda
    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 373, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/functional.py", line 1682, in softmax
    ret = input.softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 79.14 GiB total capacity; 12.25 GiB already allocated; 1.09 GiB free; 14.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
pruning layer 2 name self_attn.q_proj
pruning layer 2 name self_attn.k_proj
pruning layer 2 name self_attn.v_proj
pruning layer 2 name self_attn.o_proj
pruning layer 2 name mlp.gate_proj
pruning layer 2 name mlp.up_proj
pruning layer 2 name mlp.down_proj
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928070 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928074 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928059 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928061 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928064 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928073 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 3928069) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928062) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928060) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 3928065) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1torch
 torch 1.10.1
torch 1.10.1
1.10.1
torch 1.10.1
torch 1.10.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]
dataset loading complete
use device  cuda:0
pruning starts
loading calibdation data
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]dataset loading complete
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
use device  cuda:0
pruning starts
loading calibdation data
pruning layer 2 name self_attn.q_proj
pruning layer 2 name self_attn.k_proj
pruning layer 2 name self_attn.v_proj
pruning layer 2 name self_attn.o_proj
pruning layer 2 name mlp.gate_proj
pruning layer 2 name mlp.up_proj
pruning layer 2 name mlp.down_proj
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]pruning layer 3 name self_attn.q_proj
pruning layer 3 name self_attn.k_proj
pruning layer 3 name self_attn.v_proj
pruning layer 3 name self_attn.o_proj
pruning layer 3 name mlp.gate_proj
pruning layer 3 name mlp.up_proj
pruning layer 3 name mlp.down_proj
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1.54 GiB already allocated; 69.31 MiB free; 1.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 79.14 GiB total capacity; 1.72 GiB already allocated; 5.31 MiB free; 1.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1.54 GiB already allocated; 5.31 MiB free; 1.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 79.14 GiB total capacity; 2.41 GiB already allocated; 5.31 MiB free; 2.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 79.14 GiB total capacity; 2.41 GiB already allocated; 5.31 MiB free; 2.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 69, in main
    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
  File "/Users/918839576/Trepo/wanda_replicate/lib/prune.py", line 205, in prune_wanda
    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 339, in forward
    query_states = self.q_proj(hidden_states)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1120, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.80s/it]Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]

Traceback (most recent call last):
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1022.04 MiB already allocated; 1.31 MiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
pruning layer 4 name self_attn.q_proj
pruning layer 4 name self_attn.k_proj
pruning layer 4 name self_attn.v_proj
pruning layer 4 name self_attn.o_proj
pruning layer 4 name mlp.gate_proj
pruning layer 4 name mlp.up_proj
pruning layer 4 name mlp.down_proj
use device  cuda:0
pruning starts
loading calibdation data
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928383 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928387 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928386 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928373) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
dataset loading complete
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928372) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928375 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928374) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 3928377) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
torch 1.10.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
transformers 4.46.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
accelerate 1.0.1
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
# of gpus:  4
loading llm model meta-llama/Llama-2-7b-chat-hf
/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.63s/it]use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]
use device  cuda:0
pruning starts
loading calibdation data
dataset loading complete
dataset loading complete
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]dataset loading complete
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.q_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name self_attn.k_proj
pruning layer 0 name mlp.down_proj
pruning layer 0 name self_attn.v_proj
pruning layer 0 name self_attn.o_proj
pruning layer 0 name mlp.gate_proj
pruning layer 0 name mlp.up_proj
pruning layer 0 name mlp.down_proj
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]
use device  cuda:0
pruning starts
loading calibdation data
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 250.01 MiB already allocated; 55.31 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 1.84 GiB already allocated; 55.31 MiB free; 1.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 79.14 GiB total capacity; 0 bytes already allocated; 43.31 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 69, in main
    prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)
  File "/Users/918839576/Trepo/wanda_replicate/lib/prune.py", line 205, in prune_wanda
    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 339, in forward
    query_states = self.q_proj(hidden_states)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1120, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928665 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928673 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928677 closing signal SIGTERM
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 250.01 MiB already allocated; 24.88 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 250.01 MiB already allocated; 24.88 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 250.01 MiB already allocated; 24.88 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
        return model_class.from_pretrained(return model_class.from_pretrained(

  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
        ) = cls._load_pretrained_model() = cls._load_pretrained_model(

  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
        new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(

  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 336.01 MiB already allocated; 24.88 MiB free; 338.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 250.01 MiB already allocated; 24.88 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 336.01 MiB already allocated; 24.88 MiB free; 338.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards:   0%|          | 0/2 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 110, in <module>
    main()
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 57, in main
    model = get_llm(args.model, args.cache_dir)
  File "/Users/918839576/Trepo/wanda_replicate/main.py", line 17, in get_llm
    model = AutoModelForCausalLM.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4225, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4728, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py", line 993, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 79.14 GiB total capacity; 336.01 MiB already allocated; 24.88 MiB free; 338.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.q_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name self_attn.k_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name self_attn.v_proj
pruning layer 1 name self_attn.o_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.gate_proj
pruning layer 1 name mlp.down_proj
pruning layer 1 name mlp.up_proj
pruning layer 1 name mlp.down_proj
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 3928669) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928667 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928671 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928675 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928664) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928662 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3928666 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 3928670) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
ERROR:torch.distributed.elastic.agent.server.api:Error waiting on exit barrier. Elapsed: 6.073354244232178 seconds
Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 899, in _exit_barrier
    store_util.barrier(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py", line 67, in barrier
    synchronize(store, data, rank, world_size, key_prefix, barrier_timeout)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py", line 53, in synchronize
    agent_data = get_all(store, key_prefix, world_size)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py", line 31, in get_all
    data = store.get(f"{prefix}{idx}")
RuntimeError: Stop_waiting response is expected
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3928663) of binary: /Users/918839576/miniconda3/envs/prune_llm/bin/python
Exception in thread RendezvousKeepAliveTimer_0:
Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/utils.py", line 255, in _run
    ctx.function(*ctx.args, **ctx.kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 1133, in _keep_alive_weak
    self._keep_alive()
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 1143, in _keep_alive
    self._op_executor.run(op, deadline)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 606, in run
ERROR:torch.distributed.elastic.agent.server.api:Error waiting on exit barrier. Elapsed: 0.5466306209564209 seconds
Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 899, in _exit_barrier
    store_util.barrier(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py", line 67, in barrier
    synchronize(store, data, rank, world_size, key_prefix, barrier_timeout)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py", line 53, in synchronize
    agent_data = get_all(store, key_prefix, world_size)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py", line 31, in get_all
    data = store.get(f"{prefix}{idx}")
RuntimeError: Stop_waiting response is expected
    has_set = self._state_holder.sync()
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 408, in sync
    get_response = self._backend.get_state()
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 73, in get_state
    base64_state: bytes = self._call_store("get", self._key)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 113, in _call_store
    return getattr(self._store, store_op)(*args, **kwargs)
MemoryError: std::bad_alloc
WARNING:torch.distributed.elastic.rendezvous.dynamic_rendezvous:The node 'gpu01.hpc.at.sfsu.edu_3927706_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/Users/918839576/miniconda3/envs/prune_llm/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.10.1', 'console_scripts', 'torchrun')())
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/run.py", line 719, in main
    run(args)
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/Users/918839576/miniconda3/envs/prune_llm/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-01_04:08:58
  host      : gpu01.hpc.at.sfsu.edu
  rank      : 12 (local_rank: 0)
  exitcode  : 1 (pid: 3928663)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[63507,1],3]
  Exit code:    1
--------------------------------------------------------------------------
Finished wanda pruning with MPI on a single node
